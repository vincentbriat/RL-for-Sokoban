{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3FsKIDmu28YE"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import gym\n",
        "import tensorflow as tf\n",
        "# import tensorflow_probability as tfp\n",
        "from collections import deque\n",
        "import random\n",
        "import time\n",
        "from IPython import display\n",
        "import warnings\n",
        "import sys\n",
        "from io import StringIO\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vEsfBVFhHMRa"
      },
      "source": [
        "# Sokoban"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "3bdMfLt3HGxs"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "from gym import spaces\n",
        "\n",
        "class SokobanEnv(gym.Env):\n",
        "    metadata = {'render.modes': ['human']}\n",
        "\n",
        "    def __init__(self, level=1):\n",
        "        super(SokobanEnv, self).__init__()\n",
        "        self.setup_level(level)\n",
        "        self.height, self.width = self.map.shape\n",
        "        self.player_position = self.find_player_position()\n",
        "        self.action_space = spaces.Discrete(4)  # Four actions: 0=up, 1=down, 2=left, 3=right\n",
        "        self.observation_space = spaces.Box(low=0, high=3, shape=(self.height, self.width), dtype=np.uint8)\n",
        "        self.steps = 0\n",
        "        self.max_steps = 100  # Define maximum number of steps per episode\n",
        "        self.memory = []\n",
        "        self.use_memory = True\n",
        "\n",
        "    def map_to_str(self, map):\n",
        "      return \"\\n\".join(''.join(row) for row in map) + \"\\n\"\n",
        "\n",
        "    def map_to_int(self, map):\n",
        "        # Define mapping from characters to integers\n",
        "        char_to_int = {\n",
        "            ' ': 0,  # Empty space\n",
        "            '#': 1,  # Wall\n",
        "            'O': 2,  # Target location for boxes\n",
        "            '@': 3,  # Player\n",
        "            '$': 4,  # Box,\n",
        "            '!': 5   # Target location reached (box on top)\n",
        "        }\n",
        "\n",
        "        # Normalize values\n",
        "        max_value = max(char_to_int.values())\n",
        "        char_to_int = {char: value / max_value for char, value in char_to_int.items()}\n",
        "\n",
        "        # Convert each character in the map to its corresponding integer value\n",
        "        int_map = [[char_to_int[char] for char in row] for row in map]\n",
        "\n",
        "        return np.array(int_map)\n",
        "\n",
        "    def setup_level(self, level: int):\n",
        "        self.level = level\n",
        "        self.map = self.load_level(level)\n",
        "\n",
        "    def load_level(self, level):\n",
        "        return np.array([\n",
        "            ['#', '#', '#', '#', '#', '#', '#', '#', '#', '#'],\n",
        "            ['#', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', '#'],\n",
        "            ['#', ' ', 'O', ' ', ' ', ' ', ' ', ' ', ' ', '#'],\n",
        "            ['#', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', '#'],\n",
        "            ['#', ' ', ' ', ' ', ' ', ' ', ' ', '$', ' ', '#'],\n",
        "            ['#', ' ', ' ', ' ', ' ', ' ', ' ', ' ', '@', '#'],\n",
        "            ['#', '#', '#', '#', '#', '#', '#', '#', ' ', '#'],\n",
        "            [' ', ' ', ' ', ' ', ' ', ' ', ' ', '#', '#', '#']\n",
        "        ])\n",
        "\n",
        "    def find_player_position(self):\n",
        "        return np.array(np.where(self.map == '@'))[:, 0]\n",
        "\n",
        "    def step(self, action):\n",
        "        done = False\n",
        "        self.steps += 1\n",
        "        new_player_position = self.player_position.copy()\n",
        "\n",
        "        if action == 0:  # Move up\n",
        "            new_player_position -= [1, 0]\n",
        "        elif action == 1:  # Move down\n",
        "            new_player_position += [1, 0]\n",
        "        elif action == 2:  # Move left\n",
        "            new_player_position -= [0, 1]\n",
        "        elif action == 3:  # Move right\n",
        "            new_player_position += [0, 1]\n",
        "\n",
        "        # Check if the new position is valid\n",
        "        if self.map[tuple(new_player_position)] == '#':  # Wall\n",
        "            reward = -.1\n",
        "        elif self.map[tuple(new_player_position)] in [' ', 'O']:  # Empty space\n",
        "            self.map[tuple(self.player_position)] = ' '\n",
        "            self.map[tuple(new_player_position)] = '@'\n",
        "            self.player_position = new_player_position\n",
        "            reward = 0.01\n",
        "        elif self.map[tuple(new_player_position)] == '$':  # Box\n",
        "            box_new_position = new_player_position + (new_player_position - self.player_position)\n",
        "            if self.map[tuple(box_new_position)] in ['#', '$']:  # Box cannot be moved\n",
        "                reward = -.1\n",
        "            elif self.map[tuple(box_new_position)] == 'O': # Box reached the right destination\n",
        "                self.map[tuple(self.player_position)] = ' '\n",
        "                self.map[tuple(new_player_position)] = '@'\n",
        "                self.map[tuple(box_new_position)] = '!'\n",
        "                reward = 1\n",
        "                done = True\n",
        "            else:\n",
        "                self.map[tuple(self.player_position)] = ' '\n",
        "                self.map[tuple(new_player_position)] = '@'\n",
        "                self.map[tuple(box_new_position)] = '$'\n",
        "                self.player_position = new_player_position\n",
        "                reward = .2\n",
        "\n",
        "        done = done or self.steps >= self.max_steps  # Game is done if the player reaches the target or maximum steps reached\n",
        "\n",
        "        if self.use_memory:\n",
        "          self.memory.append(self.map.copy())\n",
        "\n",
        "        return self.map_to_int(self.map), reward, done, {}\n",
        "\n",
        "    def reset(self):\n",
        "        self.map = self.load_level(self.level)\n",
        "        self.player_position = self.find_player_position()\n",
        "        self.steps = 0\n",
        "        self.memory = [self.map.copy()]\n",
        "        return self.map_to_int(self.map)\n",
        "\n",
        "    def render(self, mode='human'):\n",
        "        if mode == 'human':\n",
        "            display.clear_output(wait=True)\n",
        "            print(self.map_to_str(self.map))\n",
        "        else:\n",
        "            outfile = sys.stdout\n",
        "            outfile.write(self.map_to_str(self.map))\n",
        "            return outfile\n",
        "\n",
        "    def play_memory(self, framerate=2):\n",
        "        for i, map in enumerate(self.memory):\n",
        "            display.clear_output(wait=True)\n",
        "            print(\"\\n\".join(''.join(row) for row in map) + \"\\n\")\n",
        "            if i != len(self.memory)-1:\n",
        "              time.sleep(1/framerate)\n",
        "        print(\"Replay over!\")\n",
        "\n",
        "    # Loads a game recorded on a txt file in memory\n",
        "    def load_txt(self, filepath):\n",
        "        with open(filepath, \"r\") as f:\n",
        "            data = f.read()\n",
        "            data = data.split('\\n-\\n')\n",
        "            if data:\n",
        "              self.reset()\n",
        "              self.memory = []\n",
        "            for map_str in data:\n",
        "                if map_str:\n",
        "                  map = map_str.split('\\n')\n",
        "                  self.memory.append(np.array([list(row) for row in map]))\n",
        "\n",
        "    # Save a game in memory on a txt file\n",
        "    def save_txt(self, filepath):\n",
        "        if not self.memory:\n",
        "            print(f\"The memory is empty, nothing was saved in {filepath}\")\n",
        "        with open(filepath, \"w\") as f:\n",
        "            for map in self.memory:\n",
        "              f.write(self.map_to_str(map))\n",
        "              f.write(\"-\\n\")\n",
        "        print(f\"The last game was successfully saved in {filepath}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Cs1F2dDH1fg",
        "outputId": "72f8a23c-493d-400b-ff18-2596b5cddc3f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "##########\n",
            "#        #\n",
            "# O      #\n",
            "#        #\n",
            "#      $ #\n",
            "#       @#\n",
            "######## #\n",
            "       ###\n",
            "\n"
          ]
        }
      ],
      "source": [
        "game = SokobanEnv()\n",
        "game.reset()\n",
        "game.render()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LGzZOAWIB4Ca",
        "outputId": "bc1d4f01-d76a-48bf-8bc3-925448764a52"
      },
      "outputs": [],
      "source": [
        "# game.save_txt(\"test.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "id": "bFWIzrRhAeGP",
        "outputId": "67ef19c6-db0b-42f2-9ffd-019a8756c04b"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'test.txt'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-2f2fedaafc6a>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_txt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"test.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplay_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-13a682941a17>\u001b[0m in \u001b[0;36mload_txt\u001b[0;34m(self, filepath)\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[0;31m# Loads a game recorded on a txt file in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_txt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n-\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'test.txt'"
          ]
        }
      ],
      "source": [
        "game.load_txt(\"test.txt\")\n",
        "game.play_memory()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_-xOh4iHPra"
      },
      "source": [
        "# Deep Q learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "HBBdwUkhHjkx"
      },
      "outputs": [],
      "source": [
        "obs_count = game.map.size\n",
        "action_count = game.action_space.n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "JAvVp3mD3bda"
      },
      "outputs": [],
      "source": [
        "alpha = 0.001\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Conv2D(64, kernel_size=(2, 2), activation='relu', input_shape=game.map.shape+(1,)),\n",
        "    keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "    # keras.layers.Conv2D(64, kernel_size=(2, 2), activation='relu'),\n",
        "    # keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "    keras.layers.Flatten(),\n",
        "    keras.layers.Dense(8, activation='relu'),\n",
        "    keras.layers.Dense(action_count, activation = 'softmax')\n",
        "])\n",
        "# model.add(keras.layers.Dense(24, input_dim=obs_count, activation='relu'))\n",
        "# model.add(keras.layers.Dense(24, activation='relu'))\n",
        "# model.add(keras.layers.Dense(action_count, activation = 'linear'))\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer=keras.optimizers.Adam(learning_rate=alpha))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VDdHUKoveMQh",
        "outputId": "9a2e59ea-521a-44d9-e4c4-c4c4a0bce9af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 227ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([[-0.00669413,  0.02308736, -0.04499338,  0.038502  ]],\n",
              "      dtype=float32)"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.predict(np.expand_dims(game.map_to_int(game.memory[0]), axis=0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ropQXdX8PHZm"
      },
      "outputs": [],
      "source": [
        "def experience_replay(model, batch_size, gamma, memory, obs_count, action_count, epoch_count, loss):\n",
        "    for _ in range(epoch_count):\n",
        "        batch = random.sample(memory, batch_size)      #sample a batch from the experience memory using batch_size\n",
        "        batch_vector = np.array(batch, dtype=object)   #vectorise the batch\n",
        "\n",
        "        obs_t = np.zeros((batch_size,) + batch_vector[0,0].shape)     #observation at t: create a numpy array of zeros with dimensions batch_size, state_count\n",
        "        obs_t_next = np.zeros((batch_size,) + batch_vector[0,3].shape) #observation at t+1: create a numpy array of zeros with dimensions batch_size, state_count\n",
        "        for i in range(len(batch_vector)):             #loop through the batch collecting the obs at t and obs at t+1\n",
        "            obs_t[i] = batch_vector[i,0]               #store the observations at t in the relevant array\n",
        "            obs_t_next[i] = batch_vector[i,3]          #store the observations at t+1 in the relevant array\n",
        "\n",
        "        prediction_at_t = model.predict(obs_t, verbose=0)         #Use the model to predict an action using observations at t\n",
        "        prediction_at_t_next = model.predict(obs_t_next, verbose=0)  #Use the model to predict an action using observations at t+1\n",
        "\n",
        "        #Create the features(X) and lables(y)\n",
        "        X = []                   #This is our feature vector, the most recent observation\n",
        "        y = []                   #This is our label calculated using the long term discounted reward\n",
        "        i = 0\n",
        "        for obs_t, action, reward, _, done in batch_vector: #get a row from our batch\n",
        "\n",
        "            X.append(obs_t)                          #append the most recent observation to X\n",
        "\n",
        "            if done:                                 #if the episode was over\n",
        "                target = reward                      #the target value is just the reward\n",
        "            else:                                    #otherwise\n",
        "                #the target value is the discounted optimal reward (Bellman optimality equation)\n",
        "                #Remember we use the max action_value from the state(observation) at time t+1\n",
        "                target = reward + gamma * np.max(prediction_at_t_next[i])\n",
        "\n",
        "            #now we update the action value for the original state(observation) given the action that\n",
        "            #was taken, and we use the target value as the update.\n",
        "            prediction_at_t[i,action] = target\n",
        "            #the updated action values are used as the label\n",
        "            y.append(prediction_at_t[i])    #Remember the update will be used as the label to update the ANN weights\n",
        "                                            #by backpropagating the mean squared error.\n",
        "\n",
        "            i += 1                          #increment i\n",
        "\n",
        "        h, w = batch_vector[0,0].shape\n",
        "        X_train = np.array(X).reshape(batch_size, h, w)          #reshape X\n",
        "        y_train = np.array(y)                                   #create a numpy array from y\n",
        "        hist = model.fit(X_train, y_train, epochs = 1, verbose = 0) #fit the model with X,y and epochs\n",
        "        for i in range(1):                             #loop over the epoch_count\n",
        "            loss.append(hist.history['loss'][i])                 #record the loss for analysis\n",
        "\n",
        "    return loss                                              #return the loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iOMuvoMw7Gui"
      },
      "outputs": [],
      "source": [
        "rewards = []                                              #store the rewards in a list for analysis only\n",
        "loss = []                                                 #store the losses in a list for analysis only\n",
        "episodes = 10                                             #We will start with 50 episodes and see how it goes\n",
        "gamma = 0.3                                               #This is the discount rate\n",
        "beta = 0.6                                                #This is the epsilon decay rate\n",
        "batch_size = 8                                           #The batch size for solving the IID problem\n",
        "memory = deque([], maxlen=250)                           #The memory replay buffer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ixOT4FKT7INE"
      },
      "outputs": [],
      "source": [
        "BEST_ACTIONS = [0]*1+[2]*5+[1]+[2]+[0]*2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rL3m6fC_3BoK",
        "outputId": "c0c0b522-5626-4621-9650-23d0d13665fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "episode: 0/10, score: 2.23, epsilon: 1.0\n",
            "episode: 1/10, score: -2.4200000000000053, epsilon: 0.8695652173913044\n",
            "episode: 2/10, score: -2.470000000000013, epsilon: 0.7692307692307692\n",
            "episode: 3/10, score: 2.23, epsilon: 0.6896551724137931\n",
            "episode: 4/10, score: -2.3600000000000034, epsilon: 0.625\n",
            "episode: 5/10, score: -4.340000000000005, epsilon: 0.5714285714285714\n",
            "episode: 6/10, score: 2.23, epsilon: 0.5263157894736842\n",
            "episode: 7/10, score: -7.469999999999993, epsilon: 0.48780487804878053\n",
            "episode: 8/10, score: -5.879999999999999, epsilon: 0.45454545454545453\n",
            "episode: 9/10, score: 2.23, epsilon: 0.4255319148936171\n"
          ]
        }
      ],
      "source": [
        "for episode in range(episodes):\n",
        "    obs_t = game.reset()\n",
        "    obs_t = np.expand_dims(obs_t, axis=0)\n",
        "    # obs_t = np.reshape(obs_t, [1, obs_count]) # Flattens the map for the Dense layers\n",
        "\n",
        "    total_reward = 0\n",
        "    epsilon = .1\n",
        "    epsilon = 1 / (1 + beta * (episode / action_count)) # Decreasing level of exploration over episodes\n",
        "\n",
        "    i = 0\n",
        "\n",
        "    done = False\n",
        "    while not done:\n",
        "        if episode%3==0:\n",
        "          # Follows the \"best actions\"\n",
        "          if i < len(BEST_ACTIONS):\n",
        "              action = BEST_ACTIONS[i]\n",
        "              i += 1\n",
        "          else:\n",
        "              action = game.action_space.sample()\n",
        "\n",
        "        else:\n",
        "          # Epsilon-greedy policy\n",
        "          rand_num = np.random.random()\n",
        "          if rand_num <= epsilon:\n",
        "              action = game.action_space.sample() # Random action\n",
        "          else:\n",
        "              action_values = model.predict(obs_t, verbose=0)\n",
        "              action = np.argmax(action_values[0]) # Best choice\n",
        "\n",
        "\n",
        "        obs_t_next, reward, done, info = game.step(action)\n",
        "        obs_t_next = np.expand_dims(obs_t_next, axis=0)\n",
        "        # obs_t_next = np.reshape(obs_t_next, [1, obs_count]) # Flattens new state\n",
        "        total_reward += reward\n",
        "        memory.append((obs_t[0], action, reward, obs_t_next[0], done))\n",
        "        obs_t = obs_t_next\n",
        "\n",
        "        if done:\n",
        "            rewards.append(total_reward)\n",
        "            print(f'episode: {episode}/{episodes}, score: {total_reward}, epsilon: {epsilon}')\n",
        "\n",
        "        if len(memory) > batch_size:\n",
        "            loss = experience_replay(model, batch_size, gamma, memory, obs_count, action_count, 2, loss)\n",
        "\n",
        "    rewards.append(total_reward)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g2kfaFP43ieB",
        "outputId": "e0da8599-4806-4eef-ed4d-d9362f19bdbf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "episode: 0/1, Total Reward: -9.99999999999998\n"
          ]
        }
      ],
      "source": [
        "test_episodes = 1\n",
        "all_rewards = []\n",
        "for episode in range(test_episodes):\n",
        "    obs_t = game.reset()\n",
        "    obs_t = np.expand_dims(obs_t, axis=0)\n",
        "    # obs_t = np.reshape(obs_t, [1, obs_count])\n",
        "    total_reward = 0\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        action_values = model.predict(obs_t, verbose=0)\n",
        "        action = np.argmax(action_values[0])\n",
        "        obs_t_next, reward, done, _ = game.step(action)\n",
        "        obs_t_next = np.expand_dims(obs_t_next, axis=0)\n",
        "        # obs_t_next = np.reshape( obs_t_next, [1, obs_count])\n",
        "        total_reward += reward\n",
        "\n",
        "        obs_t = obs_t_next\n",
        "\n",
        "        if done:\n",
        "            all_rewards.append(total_reward)\n",
        "            print(f'episode: {episode}/{test_episodes}, Total Reward: {total_reward}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K29NPxPn968y",
        "outputId": "7af9d6fa-2bf4-4bac-d1a6-142cfe1ec1a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 22ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([[ 0.16398616,  0.4590819 , -0.83621997,  0.03082588]],\n",
              "      dtype=float32)"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.predict(np.expand_dims(game.map_to_int(game.memory[0]), axis=0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ydFNiSN2aPUD",
        "outputId": "776df262-0c13-45ba-b11b-68e0369cc0f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "##########\n",
            "#        #\n",
            "# O      #\n",
            "#        #\n",
            "#      $ #\n",
            "#        #\n",
            "########@#\n",
            "       ###\n",
            "\n",
            "Replay over!\n"
          ]
        }
      ],
      "source": [
        "game.play_memory()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UejH50qw6hFc"
      },
      "source": [
        "# Best scenario"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cOOm4tI91AC4",
        "outputId": "7c657c5c-ba03-4bab-800e-1f8331f11da8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "##########\n",
            "#        #\n",
            "# !      #\n",
            "# @      #\n",
            "#        #\n",
            "#        #\n",
            "######## #\n",
            "       ###\n",
            "\n",
            "Replay over!\n"
          ]
        }
      ],
      "source": [
        "game.reset()\n",
        "game.step(0) # Up\n",
        "game.step(2) # Left\n",
        "game.step(2) # Left\n",
        "game.step(2) # Left\n",
        "game.step(2) # Left\n",
        "game.step(2) # Left\n",
        "game.step(1) # Down\n",
        "game.step(2) # Left\n",
        "game.step(0) # Up\n",
        "game.step(0) # Up\n",
        "\n",
        "game.play_memory()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXOtFMMcUuxA"
      },
      "source": [
        "# Actor-Critic Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ActorCriticNN(tf.keras.Model):\n",
        "    def __init__(self, game, n_actions, fct1_dims = 64, fct2_dims = 8,\n",
        "                 name='actor_critic', chkpt_dir = 'tmp/actor_critic'):\n",
        "        super(ActorCriticNN, self).__init__()\n",
        "        self.n_actions = n_actions\n",
        "        self.model_name = name\n",
        "        self.checkpoint_dir = chkpt_dir\n",
        "        self.checkpoint_file = os.path.join(self.checkpoint_dir, name+'_ac')\n",
        "        \n",
        "        self.flatten = tf.keras.layers.Flatten()\n",
        "        self.dense1 = tf.keras.layers.Dense(fct1_dims, activation='relu', input_shape=game.map.shape+(1,), name = 'Feature Extraction')\n",
        "        self.dense2 = tf.keras.layers.Dense(fct2_dims, activation='relu', name='Hidden layer 1')\n",
        "        self.v = tf.keras.layers.Dense(1, activation=None, name='Value head')\n",
        "        self.pi = tf.keras.layers.Dense(n_actions, activation = 'softmax', name='Policy head')\n",
        "    \n",
        "    def call(self, state):\n",
        "        value = self.conv1(state)\n",
        "        value = self.mp1(value)\n",
        "        value = self.flatten(value)\n",
        "        value = self.dense1(value)\n",
        "\n",
        "        v = self.v(value)\n",
        "        pi = self.pi(value)\n",
        "        \n",
        "        return v, pi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Agent:\n",
        "    def __init__(self, n_actions, alpha=3e-4, gamma=0.99):\n",
        "        self.gamma = gamma\n",
        "        self.alpha = alpha\n",
        "        self.action = None\n",
        "        \n",
        "        self.actor_critic = ActorCriticNN(game, n_actions=n_actions)\n",
        "\n",
        "        self.actor_critic.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=alpha))\n",
        "\n",
        "    def choose_action(self, observation):\n",
        "        state = tf.convert_to_tensor([observation])\n",
        "        _,probs = self.actor_critic(state)\n",
        "        probs = tf.squeeze(probs)\n",
        "\n",
        "        action = np.random.choice(len(probs), p=probs)\n",
        "        self.action = action\n",
        "\n",
        "        return action\n",
        "    \n",
        "    def save_models(self):\n",
        "        print('--------- Saving Models ---------')\n",
        "        self.actor_critic.save_weights(self.actor_critic.checkpoint_file)\n",
        "\n",
        "    def load_models(self):\n",
        "        print('--------- Loading Models ---------')\n",
        "        self.actor_critic.load_weights(self.actor_critic.checkpoint_file)\n",
        "\n",
        "    def learn(self, state, reward, state_, done):\n",
        "        state = tf.convert_to_tensor([state], dtype=tf.float32)\n",
        "        state_ = tf.convert_to_tensor([state_], dtype=tf.float32)\n",
        "        reward = tf.convert_to_tensor([reward], dtype=tf.float32)\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            state_value, probs = self.actor_critic(state)\n",
        "            state_value_, _ = self.actor_critic(state_)\n",
        "            state_value = tf.squeeze(state_value)\n",
        "            probs = tf.squeeze(probs)\n",
        "            state_value_ = tf.squeeze(state_value_)\n",
        "\n",
        "            log_prob = np.log(probs[self.action])\n",
        "\n",
        "            delta = reward + self.gamma * state_value_*(1-int(done)) - state_value\n",
        "            actor_loss = -log_prob*delta\n",
        "            critic_loss = delta**2\n",
        "\n",
        "            total_loss = actor_loss + critic_loss\n",
        "\n",
        "        gradient = tape.gradient(total_loss, self.actor_critic.trainable_variables)\n",
        "        self.actor_critic.optimizer.apply_gradients(zip(\n",
        "            gradient,\n",
        "            self.actor_critic.trainable_variables\n",
        "        ))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "agent = Agent(alpha=1e-5, n_actions=game.action_space.n)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "ename": "InvalidArgumentError",
          "evalue": "Exception encountered when calling Conv2D.call().\n\n\u001b[1m{{function_node __wrapped__AddV2_device_/job:localhost/replica:0/task:0/device:CPU:0}} Incompatible shapes: [1,1,8,10] vs. [1,1,1,64] [Op:AddV2] name: \u001b[0m\n\nArguments received by Conv2D.call():\n  • inputs=tf.Tensor(shape=(1, 1, 8, 10), dtype=float32)",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[10], line 16\u001b[0m\n\u001b[0;32m     13\u001b[0m score \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;66;03m# Observation must be of shape [channels, height, width]\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchoose_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpand_dims\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m     observation_, reward, done, info \u001b[38;5;241m=\u001b[39m game\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m     18\u001b[0m     score \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n",
            "Cell \u001b[1;32mIn[8], line 13\u001b[0m, in \u001b[0;36mAgent.choose_action\u001b[1;34m(self, observation)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mchoose_action\u001b[39m(\u001b[38;5;28mself\u001b[39m, observation):\n\u001b[0;32m     12\u001b[0m     state \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconvert_to_tensor([observation])\n\u001b[1;32m---> 13\u001b[0m     _,probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactor_critic\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m     probs \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39msqueeze(probs)\n\u001b[0;32m     16\u001b[0m     action \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(\u001b[38;5;28mlen\u001b[39m(probs), p\u001b[38;5;241m=\u001b[39mprobs)\n",
            "File \u001b[1;32mc:\\Users\\Vincent\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:123\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    120\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    122\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 123\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    125\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
            "Cell \u001b[1;32mIn[7], line 18\u001b[0m, in \u001b[0;36mActorCriticNN.call\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall\u001b[39m(\u001b[38;5;28mself\u001b[39m, state):\n\u001b[1;32m---> 18\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmp1(value)\n\u001b[0;32m     20\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflatten(value)\n",
            "\u001b[1;31mInvalidArgumentError\u001b[0m: Exception encountered when calling Conv2D.call().\n\n\u001b[1m{{function_node __wrapped__AddV2_device_/job:localhost/replica:0/task:0/device:CPU:0}} Incompatible shapes: [1,1,8,10] vs. [1,1,1,64] [Op:AddV2] name: \u001b[0m\n\nArguments received by Conv2D.call():\n  • inputs=tf.Tensor(shape=(1, 1, 8, 10), dtype=float32)"
          ]
        }
      ],
      "source": [
        "n_games = 10\n",
        "best_score = -100\n",
        "score_history = []\n",
        "load_checkpoint = False\n",
        "train_model = True\n",
        "\n",
        "if load_checkpoint:\n",
        "    agent.load_models()\n",
        "\n",
        "for i in range(n_games):\n",
        "    observation = game.reset()\n",
        "    done = False\n",
        "    score = 0\n",
        "    while not done:\n",
        "        # Observation must be of shape [channels, height, width]\n",
        "        action = agent.choose_action(np.expand_dims(observation, axis=0))\n",
        "        observation_, reward, done, info = game.step(action)\n",
        "        score += reward\n",
        "        if train_model:\n",
        "            agent.learn(np.expand_dims(observation, axis=0), reward, np.expand_dims(observation_, axis=0), done)\n",
        "        observation = observation_\n",
        "    score_history.append(score)\n",
        "    avg_score = np.mean(score_history[-100:])\n",
        "\n",
        "    if avg_score > best_score:\n",
        "        best_score = avg_score\n",
        "        if train_model:\n",
        "            agent.save_models()\n",
        "    print(f'episode {i} - score: {score:.1f} - avg score: {avg_score:.1f}')\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
